---
title: "STAT 331 Portfolio"
author: "Ayile Locoh Donou!"
format: html 
embed-resources: true
layout: margin-left
editor: visual
execute: 
  eval: false
  echo: true
---

[**My Grade:**]{.underline} I believe my grade equivalent to course work evidenced below to be an B.

[**Learning Objective Evidence:**]{.underline} In the code chunks below, provide code from Lab or Challenge assignments where you believe you have demonstrated proficiency with the specified learning target. Be sure to specify **where** the code came from (e.g., Lab 4 Question 2).

## Working with Data

**WD-1: I can import data from a *variety* of formats (e.g., csv, xlsx, txt, etc.).**

-   `csv` Example 1 (From Lab 2)

```{r}
#| label: wd-1-csv-1
# Lab 2
surveys <- read.csv("~/Documents/YM STAT 331/Week 2/Lab 2/surveys.csv")

```

-   `csv` Example 2 (From Lab 3)

```{r}
#| label: wd-1-csv-2
# Lab 3
cat("Loading teacher_evals.csv from YM STAT 331 / Week 3 / Lab 3\n")
data <- read_csv(here("teacher_evals.csv"))

```

-   `xlsx` (

```{r}
#| label: wd-1-xlsx
# Practice activity 4 
library(tidyverse)
library(readxl)
military <- read_xlsx("gov_spending_per_capita.xlsx",
                      sheet = "Share of Govt. spending",
                      skip  = 7,
                      n_max = 190,
                      na = c('xxx', '..', '. .')
                      )


```

**WD-2: I can select necessary columns from a dataset.**

-   Example selecting specified columns

```{r}
#| label: wd-2-ex-1
# Work from my Lab 7, i I selected only the columns directly related to fish size to create a cleaner, and more focused dataset. This change was beneficial to my code because it removes unrelated variables and makes clear summaries or visualizations easier to interpret . I was able to make these edits with a peer tutor. 
fish_size <- fish |>
  select(species, length, weight)

fish_size |> head()
```

-   Example removing specified columns

```{r}
#| label: wd-2-ex-2
# I modified my lab 5, and a select statement and within it removed columns from my full join of the data sets to match the criteria above. 
interview_full <- full_join(interview, get_fit_now_member, by = "person_id") |>
  full_join(person, by = c("person_id" = "id")) |>
  select(-address, -phone_number)
```

-   Example selecting columns based on logical values (e.g., `starts_with()`, `ends_with()`, `contains()`, `where()`)

```{r}
#| label: wd-2-ex-3
# Work from Lab 4
childcare_long <- ca_childcare |>
  select(region, study_year, mfcc_infant, mfcc_toddler, mfcc_preschool) |>
  pivot_longer(
    cols = starts_with("mfcc_"),
    names_to = "care_type",
    values_to = "median_price"
  )

```

**WD-3: I can filter rows from a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   Numeric Example 1

```{r}
#| label: wd-3-numeric-ex-1
# Work from Lab 3
surveys |>
  filter(weight > 50)

```

-   Numeric Example 2

```{r}
#| label: wd-3-numeric-ex-1
# Work from Lab 4
childcare_costs |>
  filter(mfcc_infant > 300)

```

-   Character Example 1 (any context)

```{r}
#| label: wd-3-character
crime_scene_report |>
         filter(date == ("20180115"),
         city == ("SQL City"),
         type == ("murder")
         )

```

-   Character Example 2 (example must use functions from **stringr**)

```{r}
#| label: wd-3-string
witness1 <- person |>
  filter(str_detect(address_street_name, "Northwestern Dr")) |>
  filter(address_number == max(address_number))

```

-   Date (example must use functions from **lubridate**)

```{r}
#| label: wd-3-date
# I edited my Lab 5, to include a lubridate function. 
crime_scene_report |>
  filter(
    ymd(date) == ymd("20180115"),
    city == "SQL City",
    type == "murder"
  )

```

**WD-4: I can modify existing variables and create new variables in a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   Numeric Example 1

```{r}
#| label: wd-4-numeric-ex-1
# Edited version of my example from Challenge 2
surveys <- surveys |>
  mutate(weight_kg = weight / 1000)

```

-   Numeric Example 2

```{r}
#| label: wd-4-numeric-ex-2
# From Challenge 4
childcare_costs <- childcare_costs |>
  mutate(
    center_average = (mfcc_infant + mfcc_toddler + mfcc_preschool) / 3,
    home_average   = (mc_infant + mc_toddler + mc_preschool) / 3
  )

```

-   Factor Example 1 (renaming levels)

```{r}
#| label: wd-4-factor-ex-1
# From Lab 4: renaming care_type levels with recode()
childcare_long <- childcare_long |>
  mutate(
    care_type = recode(
      care_type,
      "mfcc_infant"    = "Infant",
      "mfcc_toddler"   = "Toddler",
      "mfcc_preschool" = "Preschool"
    )
  )

```

-   Factor Example 2 (reordering levels)

```{r}
#| label: wd-4-factor-ex-2
# From Lab 4: reordering care_type factor levels
childcare_long <- childcare_long |>
  mutate(
    care_type = fct_relevel(care_type, "Infant", "Toddler", "Preschool")
  )
```

-   Character (example must use functions from **stringr**)

```{r}
#| label: wd-4-string
# From Lab 5, using stringr to modify a character variable
person <- person |>
  mutate(name = str_to_upper(name))
```

-   Date (example must use functions from **lubridate**)

```{r}
#| label: wd-4-date
# I used code from my Lab 5, and created a new column data_clean where i filtered using a lubridate function within my new column. 
crime_scene_report <- crime_scene_report %>%
  mutate(date_clean = ymd(date))

crime_scene_report %>%
  filter(
    date_clean == ymd("2018-01-15"),
    city == "SQL City",
    type == "murder"
  )
  
```

**WD-5: I can use mutating joins to combine multiple dataframes.**

-   `left_join()` Example 1

```{r}
#| label: wd-5-left-ex-1
# I took my code from Lab 5, altered it slightly to include a left join. 
interview_full <- interview |>
  left_join(
    y = get_fit_now_member,
    by = "person_id"
  ) |>
  left_join(
    y = person,
    by = c("person_id" = "id")
  )

interview_full |>
  filter(license_id == 118009)

```

-   `right_join()` Example 1

```{r}
#| label: wd-5-right
# From Lab 4 (modified)
# I modified my Lab 4 code to use right_join() so that all California counties are kept, even if some are missing childcare cost data.

ca_childcare_right <- childcare_costs |>
  right_join(
    y  = counties |> filter(state_abbreviation == "CA"),
    by = "county_fips_code"
  )

ca_childcare_right
```

-   `left_join()` **or** `right_join()` Example 2

```{r}
#| label: wd-5-left-right-ex-2
# From Lab 7, missing-value counts edited using a left join is beneficial to my code 
missing_counts_tbl <- fish |>
  map_int(~ sum(is.na(.x))) |>
  enframe(name = "variable", value = "n_missing")

var_names_tbl <- tibble(variable = names(fish))

fish_missing_joined <- var_names_tbl |>
  left_join(
    y  = missing_counts_tbl,
    by = "variable"
  )

fish_missing_joined

```

-   `inner_join()` Example 1

```{r}
#| label: wd-5-inner-ex-1
# From Lab 2, i added this to the joining species-level part as it enhances this piece of code 
species_means <- surveys |>
  group_by(species) |>
  summarise(
    mean_weight = mean(weight, na.rm = TRUE),
    .groups = "drop"
  )

surveys_with_means <- surveys |>
  inner_join(
    y  = species_means,
    by = "species"
  )

surveys_with_means

```

-   `inner_join()` Example 2

```{r}
#| label: wd-5-inner-ex-2
# From Lab 8, joining island-level flipper length 

island_summary <- penguins |>
  group_by(island) |>
  summarise(
    mean_flipper = mean(flipper_length_mm, na.rm = TRUE),
    .groups = "drop"
  )

penguins_with_summary <- penguins |>
  inner_join(
    y  = island_summary,
    by = "island"
  )

penguins_with_summary

```

**WD-6: I can use filtering joins to filter rows from a dataframe.**

-   `semi_join()`

```{r}
#| label: wd-6-semi
# From Lab 5, people who are gym members
gym_people <- person |>
  semi_join(get_fit_now_member, by = c("id" = "person_id"))

```

-   `anti_join()`

```{r}
#| label: wd-6-anti
# Edited my Lab 9, added an anti-join wich is beneficial to my code.
all_combos <- expand_grid(
  sex             = unique(df$sex),
  academic_degree = unique(df$academic_degree)
)

present_combos <- df |>
  distinct(sex, academic_degree)

missing_combos <- all_combos |>
  anti_join(
    y  = present_combos,
    by = c("sex", "academic_degree")
  )

missing_combos
```

**WD-7: I can pivot dataframes from long to wide and visa versa**

-   `pivot_longer()`

```{r}
#| label: wd-7-long
# From Lab 4, pivoting childcare costs from wide to long
recreate_plot <- ca_childcare |>
  select(region, study_year, mfcc_infant, mfcc_toddler, mfcc_preschool) |>
  pivot_longer(
    cols = starts_with("mfcc_"),
    names_to = "care_type",
    values_to = "median_price"
  ) |>
  mutate(
    care_type = recode(
      care_type,
      "mfcc_infant"    = "Infant",
      "mfcc_toddler"   = "Toddler",
      "mfcc_preschool" = "Preschool"
    )
  )
```

-   `pivot_wider()`

```{r}
#| label: wd-7-wide
# Original code from Lab  using a pivot wider function. 
pivot_table <- function(df, row, col) {
  df |>
    count({{ row }}, {{ col }}) |>
    pivot_wider(
      names_from = {{ col }},
      values_from = n,
      values_fill = 0
    ) |>
    adorn_totals(where = c("row", "col"))
}

#Edited different code with same code also using a pivot longer function. 
penguin_counts <- penguins |>
  count(species, island)

penguin_table <- penguin_counts |>
  pivot_wider(
    names_from  = island,
    values_from = n,
    values_fill = 0
  )

penguin_table

```

## Reproducibility

**R-1: I can create professional looking, reproducible analyses using RStudio projects, Quarto documents, and the here package.**

The following assignments satisfy the above criteria:

-   Example 1 - Lab 1
-   Example 2 - Lab 2
-   Example 3 - Challenge 2
-   Example 4 - Challenge 4
-   Example 5 - Lab 5

**R-2: I can write well documented and tidy code.**

-   Example of **ggplot2** plotting

```{r}
#| label: r-2-1
# My code from Lab 2. 
ggplot(data = surveys,
       mapping = aes(x = weight, y = hindfoot_length)) +
  geom_point(alpha = 0.25) +                      
  facet_wrap(~species) +                            
  labs(
    x = "Weight (g)",                               
    y = "Hindfoot length (mm)",                    
    title = "Species according to animal weight and hindfoot length",
    subtitle = "Hindfoot length (mm)"
  )

```

-   Example of **dplyr** pipeline

```{r}
#| label: r-2-2
# Work from my Lab 9 
demo_tbl <- df |>
  select(academic_degree, seniority, sex) |>
  mutate(
    academic_degree = as.character(academic_degree),
    seniority       = as.character(seniority),
    sex = recode(
      as.character(sex),
      "M" = "Male", "m" = "Male",
      "F" = "Female", "f" = "Female",
      .default = as.character(sex)
    )
  ) |>
  pivot_longer(
    cols      = everything(),
    names_to  = "Demographic",
    values_to = "Level"
  ) |>
  mutate(
    Demographic = recode(
      Demographic,
      "academic_degree" = "Academic Degree",
      "seniority"       = "Seniority",
      "sex"             = "Sex"
    )
  ) |>
  count(Demographic, Level, name = "Count") |>
  group_by(Demographic) |>
  mutate(Percent = Count / sum(Count)) |>
  ungroup() |>
  arrange(Demographic, desc(Count))

demo_tbl

```

-   Example of function formatting

```{r}
#| label: r-2-3
# I edited cide from my Lab 5 to include function formatting. 
filter_crime <- function(data, crime_date, crime_city, crime_type)(
  data %>%
    filter(
      date == crime_date,   
      city == crime_city,   
      type == crime_type    
    )
)
sql_city_murder <- filter_crime(
  data = crime_scene_report,
  crime_date = "20180115",
  crime_city = "SQL City",
  crime_type = "murder"
)

```

**R-3: I can write robust programs that are resistant to changes in inputs.**

-   Example (any context)

```{r}
#| label: r-3-example
# I revised my Lab 10 simulation function to include input checks
# and use stop() when the inputs are not valid.

simulate_ci_once <- function(beta0, beta1, n) {
  # Input checks to make the function more robust
  if (!is.numeric(beta0) || length(beta0) != 1) {
    stop("beta0 must be a single numeric value.")
  }

  if (!is.numeric(beta1) || length(beta1) != 1) {
    stop("beta1 must be a single numeric value.")
  }

  if (!is.numeric(n) || length(n) != 1 || n <= 1) {
    stop("n must be a single numeric value greater than 1.")
  }

  # Data generation (from Lab 10)
  x  <- runif(n, 0, 1)
  ep <- rnorm(n, mean = 0, sd = 1)
  y  <- beta0 + beta1 * x + ep
  dat <- tibble(x = x, y = y)

  # Fit linear model and return slope estimate + CI
  fit <- lm(y ~ x, data = dat)

  tidy(fit, conf.int = TRUE) |>
    filter(term == "x") |>
    transmute(
      estimate = estimate,
      conf.low = conf.low,
      conf.high = conf.high
    )
}

# Example use:
simulate_ci_once(beta0 = 3, beta1 = 0.5, n = 100)

```

-   Example (function stops)

```{r}
#| label: r-3-function-stops
# I revised my Lab 8 rescale_01() function to be more robust and resistant to changes using a peer tutor. It now checks inputs and uses stop() when the input is invalid.

rescale_01_checked <- function(x) {

  # Input must be numeric
  if (!is.numeric(x)) {
    stop("Input must be numeric.")
  }

  # Must contain at least two values
  if (length(x) < 2) {
    stop("Input must contain at least two numeric values.")
  }

  # Cannot be all missing values
  if (all(is.na(x))) {
    stop("Input cannot be all NA values.")
  }

  # Original Lab 8 logic
  (x - min(x, na.rm = TRUE)) / 
    (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Example use with penguins data (Lab 8)
rescale_01_checked(penguins$bill_length_mm)
```

## Data Visualization & Summarization

**DVS-1: I can create visualizations for a *variety* of variable types (e.g., numeric, character, factor, date)**

-   At least two numeric variables

```{r}
#| label: dvs-1-num
# i revised my Lab 7 with a peer tutor 
missing_summary <- fish |>
  group_by(year) |>
  summarise(
    mean_weight   = mean(weight, na.rm = TRUE),
    mean_length   = mean(length, na.rm = TRUE),
    .groups = "drop"
  )

# Step 2: Plot two numeric variables (mean_length vs. mean_weight)
ggplot(missing_summary, aes(x = mean_length, y = mean_weight)) +
  geom_point(alpha = 0.6) +
  labs(
    x = "Mean Length (mm)",
    y = "Mean Weight (g)",
    title = "Mean Fish Length vs Weight Across Years"
  ) +
  theme_minimal()


```

-   At least one numeric variable and one categorical variable

```{r}
#| label: dvs-2-num-cat
# Work from my Lab 2 edited with my peer tutor. 
surveys_clean <- surveys |>
  filter(!is.na(weight), !is.na(species))

ggplot(surveys_clean, aes(x = weight, y = species)) +
  geom_jitter(width = 0.3, alpha = 0.25, color = "green") +
  geom_boxplot(outlier.shape = NA) +
  labs(
    y = "Species",
    x = "Weight (g)",
    title = "Rodent Weights by Species (Lab 2)"
  ) +
  theme(axis.text.y = element_text(angle = 30, hjust = 1))   

```

-   At least two categorical variables

```{r}
#| label: dvs-2-cat
# I used my work from Lab 8, adjusted whats necessary and cleaned up my code 
penguins_clean <- penguins |>
  filter(!is.na(species), !is.na(sex))

ggplot(penguins_clean, aes(x = species, fill = sex)) +
  geom_bar() +
  labs(
    title = "Counts of Penguins by Species and Sex",
    x = "Species",
    y = "Count",
    fill = "Sex"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))
```

-   Dates (time series plot)

```{r}
#| label: dvs-2-date
# Used my Lab 4 and adjusted it with my peer tutor. 
year_price <- ca_childcare |>
  group_by(study_year) |>
  summarise(
    median_infant_price = median(mfcc_infant, na.rm = TRUE),
    .groups = "drop"
  )
ggplot(year_price, aes(x = study_year, y = median_infant_price)) +
  geom_line(color = "#0072B2") +
  labs(
    x = "Year",
    y = "Median Infant Price ($)",
    title = "Median Infant Childcare Price Over Time (CA)",
    subtitle = "Using Lab 4 childcare cost data"
  ) +
  scale_y_continuous(labels = scales::label_dollar()) +   # modern syntax
  theme_minimal()

```

**DVS-2: I use plot modifications to make my visualization clear to the reader.**

-   I can modify my plot theme to be more readable

```{r}
#| label: dvs-2-ex-1
# From Lab 4, adjusted with peer tutor. 

ggplot(ca_childcare, aes(x = mhi_2018, y = mfcc_infant, color = region)) +
  geom_point(alpha = 0.5, size = 0.7) +
  labs(
    title = "Infant Childcare Price vs. Household Income in California",
    x = "Median Household Income (2018 dollars)",
    y = "Weekly Price for Center-Based Infant Care",
    color = "Region"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    legend.position = "right",
    legend.title = element_text(size = 9),
    legend.text  = element_text(size = 7),
    axis.text.x  = element_text(angle = 20, hjust = 1),
    axis.text.y  = element_text(size = 8)
  )

```

-   I can modify my colors to be accessible to anyone's eyes

```{r}
#| label: dvs-2-ex-2
# From Challenge 2,i am using a colorblind-friendly palette. I chose a blue and orange palette (#0072B2 and #E69F00) because these colors have good contrast and are commonly recommended as colorblind-friendly. This makes it easier for all viewers, including those with color vision deficiencies, to distinguish between male and female rodents.

ggplot(data = surveys,
       mapping = aes(x = weight, y = species, color = sex)) +
  geom_jitter(height = 0.25, alpha = 0.125) +
  geom_boxplot(outlier.shape = NA) +
  labs(
    x = "Weight (g)",
    y = "Species",
    title = "Rodent Weight by Species and Sex",
    subtitle = "Using colorblind-friendly blue/orange palette for sex"
  ) +
  scale_color_manual(
    values = c(
      "F" = "#0072B2",  # blue
      "M" = "#E69F00"   # orange
    ),
    name = "Sex",
    labels = c("F" = "Female", "M" = "Male")
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(angle = 20, hjust = 1)
  )
```

-   I can modify my plot titles to clearly communicate the data context

```{r}
#| label: dvs-2-ex-3
# My code from Challenge 4. 
ggplot(plot_df, aes(x = setting, y = cost, fill = setting)) +
  geom_boxplot(alpha = 0.5, width = 0.5, outlier.alpha = 0.1) +
  stat_summary(fun = median, geom = "point", size = 2, color = "white") +
  scale_y_continuous(labels = scales::dollar_format(accuracy = 1)) +
  labs(
    title = "Childcare Prices by Setting in California",
    x = "Setting",
    y = "Weekly Cost of Childcare $"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "left"
  )

```

-   I can modify the text in my plot to be more readable

```{r}
#| label: dvs-2-ex-4
# From Lab 10, i adjusted my text size and labels for readability with my tutor.

ggplot(tab_df, aes(x = n_correct, y = proportion)) +
  geom_col(fill = "#D81B60") +
  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +
  labs(
    x = "Number of Babies Returned Correctly",
    y = "Proportion of Simulations",
    title = "Random Babies Simulation Results",
    subtitle = "Proportion of 10,000 simulations with 0–4 correctly matched babies"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    axis.text.x  = element_text(size = 9),
    axis.text.y  = element_text(size = 9),
    plot.title   = element_text(size = 14, face = "bold"),
    plot.subtitle= element_text(size = 10)
  )
```

-   I can reorder my legend to align with the colors in my plot

```{r}
#| label: dvs-2-ex-5
# I used my work from Lab 8, in wich i reordered the legend to match my fill colors, i did this with the help of my tutor but i understand how this betters my code. 

penguins_clean <- penguins |>
  filter(!is.na(species), !is.na(sex)) |>
  mutate(
    sex = factor(sex, levels = c("female", "male"))
  )

ggplot(penguins_clean, aes(x = species, fill = sex)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(
    values = c("female" = "#0072B2",  # blue for female
               "male"   = "#E69F00"), # orange for male
    name   = "Sex",
    labels = c("Female", "Male")
  ) +
  labs(
    title = "Number of Penguins by Species and Sex",
    x = "Species",
    y = "Count"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    axis.text.x = element_text(angle = 20, hjust = 1)
  )

```

**DVS-3: I show creativity in my visualizations**

-   I can use non-standard colors (Example 1)

```{r}
#| label: dvs-3-1-ex-1
gplot(surveys, aes(x = weight, y = species, color = sex)) +
  geom_jitter(height = 0.25, alpha = 0.125) +
  geom_boxplot(outlier.shape = NA) +
  labs(
    x = "Weight",
    y = "Species",
    title = "Rodent weight by species and sex"
  ) +
  scale_color_manual(values = c("F" = "#0072B2",   
                                "M" = "#E69F00"))
```

-   I can use non-standard colors (Example 2)

```{r}
#| label: dvs-3-1-ex-2
# This is from my From Lab 10, i edited my colors and cleaned up my work with my peer tutor. I used a set of non default hex colors with strong contrast so each bar is easy to distinguish, including for people with some color vision differences. I did this instead of using a color package. 

tab_df_colored <- tab_df |>
  mutate(n_correct = factor(n_correct))

ggplot(tab_df_colored, aes(x = n_correct, y = proportion, fill = n_correct)) +
  geom_col() +
  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +
  scale_fill_manual(
    values = c(
      "0" = "#D81B60",  
      "1" = "#1E88E5",  
      "2" = "#FFC107",  
      "3" = "#004D40",  
      "4" = "#7B1FA2"   
    ),
    name = "Correct Babies"
  ) +
  labs(
    x = "Number of Babies Returned Correctly",
    y = "Proportion of Simulations",
    title = "Random Babies Simulation with Custom Colors"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    axis.text.x = element_text(size = 9),
    axis.text.y = element_text(size = 9)
  )


```

-   I can use annotations (e.g., `geom_text()`)

```{r}
#| label: dvs-3-2
# Work from my Lab 4 edited to add annotations to highlight the year with the highest infant price, worked on this with tutor. 

year_price <- ca_childcare |>
  group_by(study_year) |>
  summarise(
    median_infant_price = median(mfcc_infant, na.rm = TRUE),
    .groups = "drop"
  )
max_year <- year_price |>
  slice_max(median_infant_price, n = 1)

ggplot(year_price, aes(x = study_year, y = median_infant_price)) +
  geom_line(color = "#0072B2") +
  geom_point(color = "#0072B2") +
  geom_point(data = max_year, color = "#D81B60", size = 2) +
  geom_text(
    data = max_year,
    aes(label = paste0("Highest: $", round(median_infant_price, 0))),
    vjust = -1,
    size = 3.2
  ) +
  labs(
    x = "Year",
    y = "Median Infant Price ($)",
    title = "Median Infant Childcare Price Over Time in California",
    subtitle = "Annotated with the year of the highest median infant price"
  ) +
  theme_minimal(base_size = 11)

```

-   I can choose creative geometries (e.g., `geom_segment()`, `geom_ribbon)()`)

```{r}
#| label: dvs-3-3
# From Lab 7 apply geom_segment() as a creative geometry, with the help of my peer tutor. I used geom_segment() to show the range of fish lengths for each year in the Lab 7 data, which i believe is a more creative way to visualize the spread than just plotting points or a single summary.


fish_length_range <- fish |>
  group_by(year) |>
  summarise(
    min_length = min(length, na.rm = TRUE),
    max_length = max(length, na.rm = TRUE),
    .groups = "drop"
  ) |>
  filter(!is.na(min_length), !is.na(max_length))

ggplot(fish_length_range, aes(x = year)) +
  geom_segment(
    aes(
      xend = year,
      y    = min_length,
      yend = max_length
    ),
    linewidth = 1
  ) +
  labs(
    title = "Range of Fish Lengths by Year",
    x     = "Year",
    y     = "Fish Length (mm)"
  ) +
  theme_minimal(base_size = 11)

```

**DVS-4: I can calculate numerical summaries of variables.**

-   Example using `summarize()`

```{r}
#| label: dvs-4-summarize
summary_table <- childcare_costs |>
  summarise(
    center_mean   = mean(c(mfcc_infant, mfcc_toddler, mfcc_preschool), na.rm = TRUE),
    center_median = median(c(mfcc_infant, mfcc_toddler, mfcc_preschool), na.rm = TRUE),
    home_mean     = mean(c(mc_infant, mc_toddler, mc_preschool), na.rm = TRUE),
    home_median   = median(c(mc_infant, mc_toddler, mc_preschool), na.rm = TRUE)
  )
view(summary_table)
```

-   Example using `across()`

```{r}
#| label: dvs-4-across
# I edited my Lab 7 to use across() to find the mean and standard deviation for every numeric column in the fish dataset in one step, instead of writing separate summarise() calls, with my peer tutor. 

fish_summary <- fish |>
  summarise(
    across(
      .cols = where(is.numeric),
      .fns  = list(
        mean = ~ mean(.x, na.rm = TRUE),
        sd   = ~ sd(.x,   na.rm = TRUE)
      ),
      .names = "{.col}_{.fn}"
    )
  )

fish_summary

```

**DVS-5: I can find summaries of variables across multiple groups.**

-   Example 1

```{r}
#| label: dvs-5-1
# From my Lab 4 I originally filtered to 2018 and then used slice_min() to find only the single region with the lowest median infant price. For this portfolio example, I removed slice_min() so that all regions are included, which provides a complete summary across groups and better demonstrates how median childcare costs vary by region, i worked on this with my peer tutor. 

region_infant_costs_2018 <- ca_childcare |>
  filter(study_year == 2018) |>
  group_by(region) |>
  summarise(
    median_infant_price = median(mfcc_infant, na.rm = TRUE),
    .groups = "drop"
  )

region_infant_costs_2018

```

-   Example 2

```{r}
#| label: dvs-5-2
# From Lab 9, the pipeline was originally used to create a formatted demographic table, but for this example I kept only the grouped counts and percentages before the table styling. This change highlights the actual summaries across demographic groups,this clearly demonstrates how each category contributes to the overall distribution.

demo_summary <- df |>
  select(academic_degree, seniority, sex) |>
  mutate(
    academic_degree = as.character(academic_degree),
    seniority       = as.character(seniority),
    sex = recode(
      as.character(sex),
      "M" = "Male", "m" = "Male",
      "F" = "Female", "f" = "Female",
      .default = as.character(sex)
    )
  ) |>
  pivot_longer(
    cols = everything(),
    names_to = "Demographic",
    values_to = "Level"
  ) |>
  mutate(
    Demographic = recode(
      Demographic,
      "academic_degree" = "Academic Degree",
      "seniority"       = "Seniority",
      "sex"             = "Sex"
    )
  ) |>
  count(Demographic, Level, name = "Count") |>
  group_by(Demographic) |>
  mutate(Percent = Count / sum(Count)) |>
  ungroup()

demo_summary

```

**DVS-6: I can create tables which make my summaries clear to the reader.**

-   I can modify my column names to clearly communicate the data context

```{r}
#| label: dvs-6-ex-1
# Work from my Lab 4, i had previously created my_table, but here I wrapped it in gt() and relabeled the columns with more descriptive names. This makes the table easier for the reader to interpret without guessing what each year column represents, additionally i worked on these edits with my tutor. 
income_table <- my_table |>
  gt() |>
  cols_label(
    region = "California Region",
    `2008` = "Median Income 2008 (2018 dollars)",
    `2018` = "Median Income 2018 (2018 dollars)"
  ) |>
  tab_header(
    title = "Median Household Income by Region and Year"
  ) |>
  fmt_number(columns = c(`2008`, `2018`), decimals = 0)

income_table

```

-   I can modify the text in my table to be more readable (e.g., bold face for column headers)

```{r}
#| label: dvs-6-ex-2
# Work from my Lab 9, i added a clearer set of column names and bolded the header row using row_spec(0, bold = TRUE). This formatting change makes the table easier to review and keeps the layout focused on the data. I was able to make these edits with the help of my peer tutor. 

demo_table <- demo_tbl |>
  kable(
    caption   = "Demographics of Instructors",
    col.names = c("Demographic", "Level", "Count", "Percent")
  ) |>
  kable_styling(full_width = FALSE) |>
  row_spec(0, bold = TRUE)

demo_table

```

-   I can arrange my table to have an intuitive ordering

```{r}
#| label: dvs-6-ex-3
# Work from my Lab 2,  here I turned the data into a clean summary table by counting observations across species and arranging them from most to least common. Ordering the species by frequency makes the table much easier to interpret because the reader can immediately see which species dominate the dataset. I was able to make these changes with the help of my peer tutor. 
species_counts <- surveys |>
  filter(!is.na(species)) |>
  count(species, name = "Count") |>
  arrange(desc(Count))

species_counts |>
  kable(
    caption   = "Number of Observations per Species (Lab 2 Surveys Data)",
    col.names = c("Species", "Number of Observations")
  )

```

**DVS-7: I show creativity in my tables.**

-   I can use non-default colors

```{r}
#| label: dvs-7-ex-1
# Work from my Lab 9, i added a clearer caption and full_width = FALSE for nicer layout. The non-default colors are beneficial in this code because they draw attention to which variables have missing data problems, wich makes my table more informative and easier to scan. I made these edits with the help of a peer tutor. 

missing_counts_tbl <- fish |>
  map_int(~ sum(is.na(.x))) |>
  enframe(name = "Variable", value = "N_Missing") |>
  arrange(desc(N_Missing), Variable)

missing_counts_tbl |>
  mutate(
    `# Missing` = if_else(
      N_Missing == 0,
      cell_spec(N_Missing, format = "html", background = "lightgreen"),
      cell_spec(N_Missing, format = "html", background = "tomato")
    )
  ) |>
  select(Variable, `# Missing`) |>
  kable(
    caption   = "Missing Values by Variable in the Fish Dataset",
    col.names = c("Variable", "# Missing")
  ) |>
  kable_styling(full_width = FALSE)


```

-   I can modify the layout of my table to be more readable (e.g., `pivot_longer()` or `pivot_wider()`)

```{r}
#| label: dvs-7-ex-2
# Work from my Lab 7, I added a final kable() call with clearer column names so the long layout created by pivot_longer() becomes a readable summary table. Using pivot_longer() in this Lab is beneficial here because it converts a wide summary into a tidy list where each variable and its missing count are in their own row, which makes the table much easier to interpret. I was able to make these edits with the help of a peer tutor. 

missing_values <- fish |>
  summarise(
    missing_observations = sum(if_any(everything(), is.na)),
    total_observations   = n(),
    across(everything(), ~ sum(is.na(.)), .names = "{.col}_n_missing")
  )

missing_tbl <- missing_values |>
  pivot_longer(
    cols = ends_with("_n_missing"),
    names_to = "Variable",
    names_pattern = "^(.*)_n_missing$",
    values_to = "N_Missing"
  ) |>
  filter(N_Missing > 0) |>
  arrange(desc(N_Missing))

missing_tbl |>
  kable(
    caption   = "Number of Missing Values for Each Fish Variable (Lab 7)",
    col.names = c("Variable", "# Missing")
  )

```

## Program Efficiency

**PE-1: I can write concise code which does not repeat itself.**

-   using a single function call with multiple inputs (rather than multiple function calls)

```{r}
#| label: pe-1-one-call
# Work from my Lab 5
crime_scene_report |>
         filter(date == ("20180115"),
         city == ("SQL City"),
         type == ("murder")
         )
```

-   using `across()`

```{r}
#| label: pe-1-across
# Work from my Lab 7, I formatted the across() function slightly differently to make the .cols, .fns, and .names arguments clearer. Using across() in this Lab is beneficial because it allows me to compute the number of missing values for all columns in one step, instead of writing separate summarise() lines for each variable, which demonstrates concise, non-repetitive code. I was able to make these changes with the help of a peer tutor. 

missing_values <- fish |>
  summarise(
    missing_observations = sum(if_any(everything(), is.na)),
    total_observations   = n(),
    across(
      .cols = everything(),
      .fns  = ~ sum(is.na(.x)),
      .names = "{.col}_n_missing"
    )
  )

missing_values

```

-   using functions from the `map()` family

```{r}
#| label: pe-1-map-1
# Work from my Lab 10, i added results[1:10] at the end so my chunk prints something small instead of the whole vector. Using map_int() is beneficial in my Lab 10 because it automatically repeats the simulation without me writting unnecessary lines of code, demonstrating how the map family reduces repetition and makes my program more efficient.
randomBabies <- function(n_babies) {
  reassigned <- sample(1:n_babies)
  correct    <- sum(reassigned == 1:n_babies)
  correct == n_babies   
}

set.seed(123)
results <- map_int(1:10000, ~ randomBabies(4))

results[1:10]

```

**PE-2: I can write functions to reduce repetition in my code.**

-   Example 1: Function that operates on vectors

```{r}
#| label: pe-2-1
#Work from Lab 8, I added a call to rescale_01(penguins$bill_length_mm) so the chunk shows how it operates directly on a vector. This function is beneficial because it lets me rescale any numeric vector to the 0–1 range with one line, instead of rewriting the min max formula every time. I was able to make these edits with a peer editor. 

rescale_01 <- function(x) {
  (x - min(x, na.rm = TRUE)) /
    (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

rescale_01(penguins$bill_length_mm)

```

-   Example 2: Function that operates on data frames

```{r}
#| label: pe-2-2
# Work from my Lab 8, I added the penguins_rescaled <- to demonstrate how it rescales multiple columns in a data frame using a single function call. This is beneficial because i am able to avoid repeating mutate() code for each column—rescale_column() plus across() handles all selected variables at once, this directly reduces repetition. I was able to make these edits with the help of a peer tutor. 
rescale_column <- function(df, cols) {
  df |>
    mutate(across({{ cols }}, rescale_01))
}

penguins_rescaled <- rescale_column(
  penguins,
  cols = c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)
)

penguins_rescaled
```

-   Example 3: Function that operates on vectors *or* data frames

```{r}
#| label: pe-2-3
# Work from my Lab 8, I added  pivot_table(penguins, row = species, col = island) to demonstrate how it can quickly produce a wide count table with row and column totals. This is beneficial to my code and final outcome function because it replaces writing separate count(), pivot_wider(), and total-calculation code every time you want a summary table. I was able to make these edits with the help of a peer tutor. 

pivot_table <- function(df, row, col) {
  df |>
    count({{ row }}, {{ col }}) |>
    pivot_wider(
      names_from  = {{ col }},
      values_from = n,
      values_fill = 0
    ) |>
    janitor::adorn_totals(where = c("row", "col"))
}

penguin_table <- pivot_table(penguins, row = species, col = island)

penguin_table
```

**PE-3:I can use iteration to reduce repetition in my code.**

-   using `across()`

```{r}
#| label: pe-3-across
# Work from my Lab 7, I spaced out the across() arguments to make it easier to see that it loops over every column once. Using the across() function here is beneficial because it applies sum(is.na()) to all variables and demonstrates iteration across columns . I was able to make these edits with the help of a peer tutor. 

missing_values <- fish |>
  summarise(
    missing_observations = sum(if_any(everything(), is.na)),
    total_observations   = n(),
    across(
      .cols  = everything(),
      .fns   = ~ sum(is.na(.x)),
      .names = "{.col}_n_missing"
    )
  )

missing_values

```

-   using a `map()` function with **one** input (e.g., `map()`, `map_chr()`, `map_dbl()`, etc.)

```{r}
#| label: pe-3-map-1
# Work from my Lab 10,i used map_dfr() to automatically run the simulation 1000 times and stack the results into a single data frame, instead of manually calling mycifun() over and over, which demonstrates iteration with one input. I was able to make these edits with the help of a peer tutor. 

mycifun <- function(beta0, beta1, n) {
  x  <- runif(n, 0, 1)
  ep <- rnorm(n, mean = 0, sd = 1)
  y  <- beta0 + beta1 * x + ep
  dat <- tibble(x = x, y = y)

  fit <- lm(y ~ x, data = dat)

  tidy(fit, conf.int = TRUE) |>
    filter(term == "x") |>
    transmute(
      estimate = estimate,
      conf.low = conf.low,
      conf.high = conf.high,
      cover = as.integer(beta1 >= conf.low & beta1 <= conf.high)
    )
}

set.seed(1000)

ci_dat <- map_dfr(
  .x = 1:1000,
  .f = ~ mycifun(beta0 = 3, beta1 = 0.5, n = 100)
)
```

-   using a `map()` function with **more than one** input (e.g., `map_2()` or `pmap()`)

```{r}
#| label: pe-3-map-2
#This is etended work from my Lab 10, i added  param_grid tibble plus pmap_dfr() so that the function is run across several combinations of beta1 and n instead of a single setting. This is beneficial because it shows how to iterate over multiple changing inputs in a concise way. I was able to make these edits with the help of a peer tutor. 

param_grid <- tibble(
  beta0 = 3,
  beta1 = c(0.3, 0.5, 0.8),
  n     = c(50, 100, 200)
)

set.seed(2025)

ci_grid <- param_grid |>
  pmap_dfr(
    .f = ~ mycifun(beta0 = ..1, beta1 = ..2, n = ..3),
    .id = "combo_id"
  )

ci_grid

```

**PE-4: I can use modern tools when carrying out my analysis.**

-   I can use functions which are not superseded or deprecated

```{r}
#| label: pe-4-1
# Work from my Lab 7, I renamed var to variable. I used pivot_longer() since it is beneficial because it’s the "modern" replacement for older, superseded functions like gather(), and it lets me reshape many “*_n_missing” columns into a tidy, readable table. I was able to make these edits with the help of a peer tutor. 

missing_values <- fish |>
  summarise(
    missing_observations = sum(if_any(everything(), is.na)),
    total_observations   = n(),
    across(everything(), ~ sum(is.na(.)), .names = "{.col}_n_missing")
  )

missing_tbl <- missing_values |>
  pivot_longer(
    cols         = ends_with("_n_missing"),
    names_to     = "variable",
    names_pattern = "^(.*)_n_missing$",
    values_to    = "n_missing"
  ) |>
  filter(n_missing > 0) |>
  arrange(desc(n_missing))

missing_tbl

```

-   I can connect a data wrangling pipeline into a `ggplot()`

```{r}
#| label: pe-4-2
#  Work from my Lab 4, I connected the select(), pivot_longer(), and mutate() steps directly into ggplot() using one pipeline. I felt this change is beneficial to my code because it keeps all the data-wrangling and plotting steps together in a single,accurate tidyverse pipeline, which makes the code easier to follow and reduces clutter from temporary objects. I was able to make these edits with the help of my peer tutor. 

ca_childcare |>
  select(region, study_year, mfcc_infant, mfcc_toddler, mfcc_preschool) |>
  pivot_longer(
    cols      = starts_with("mfcc_"),
    names_to  = "care_type",
    values_to = "median_price"
  ) |>
  mutate(
    care_type = recode(
      care_type,
      "mfcc_infant"    = "Infant",
      "mfcc_toddler"   = "Toddler",
      "mfcc_preschool" = "Preschool"
    ),
    care_type = fct_relevel(care_type, "Infant", "Toddler", "Preschool")
  ) |>
  ggplot(aes(x = study_year, y = median_price, color = care_type)) +
  geom_point(alpha = 0.7, size = 1.2) +
  geom_smooth(method = "loess", se = TRUE, linewidth = 0.7) +
  labs(
    title = "Median Center-Based Childcare Prices by Child Age Group in California",
    x     = "Year",
    y     = "Median Weekly Price ($)",
    color = "Care Type"
  ) +
  scale_x_continuous(breaks = seq(2008, 2018, 2)) +
  theme_minimal(base_size = 11)

```

## Data Simulation & Statisical Models

**DSSM-1: I can simulate data from a *variety* of probability models.**

-   Example 1

```{r}
#| label: dsm-1-1
# From Lab 10, I changed the last line to sum(reassigned == 1:n_babies) so it returns the number of correctly matched babies . This change is beneficial to my code because it gives me a simulated data from the underlying probability model and it  demonstrates that i am simulating outcomes from a random assignment process. I was able to make these changes with the help of a peer tutor. 

randomBabies <- function(n_babies) {
  reassigned <- sample(1:n_babies)           
  sum(reassigned == 1:n_babies)             
}

set.seed(123)

correct_babies <- map_int(1:10000, ~ randomBabies(4))

head(correct_babies)

```

-   Example 2

```{r}
#| label: dsm-1-2
# Work from my Lab 10, I wrapped two steps into a reusable function called simulate_linear_data() that returns a tibble. This change is beneficial to my code because it makes the simulation pattern reusable for any combination of beta0, beta1, and sample size n. I made these changes with the help of a peer tutor.
simulate_linear_data <- function(beta0, beta1, n) {
  x  <- runif(n, 0, 1)               
  ep <- rnorm(n, mean = 0, sd = 1)    
  y  <- beta0 + beta1 * x + ep        

  tibble(x = x, y = y)
}

set.seed(2024)
sim_data <- simulate_linear_data(beta0 = 2, beta1 = 1, n = 100)

head(sim_data)
```

**DSSM-2: I can conduct common statistical analyses in R.**

-   Example 1

```{r}
#| label: dsm-2-1
# Work from my Lab 2

species_mod <- aov(weight ~ species_id, data = surveys)
summary(species_mod)

```

-   Example 2

```{r}
#| label: dsm-2-2
# Work from my Lab 4, this demonstrates my ability to fit and interpret a linear model using economic data.

reg_mod1 <- lm(mfcc_infant ~ mhi_2018, data = ca_childcare)
summary(reg_mod1)

```

-   Example 3

```{r}
#| label: dsm-2-3
# Work from my Lab 10, I simplified this to demonstrate i extracted the estimated slope and its 95% confidence interval. This change was beneficial to my code because it highlights my ability to work with model output "programmatically". I was able to make these edits with the helps with my peer tutor. 
fit <- lm(y ~ x, data = observed_data)

slope_results <- tidy(fit, conf.int = TRUE)
slope_results

```

## Revising My Thinking

<!-- How did you revise your thinking throughout the course? How did you revise your thinking on the code examples you have provided in your portfolio? -->

Going through the revision process my main goal was to better understand what i did wrong, and be able to demonstrate that i understood my mistakes. A lot of my feedback surrounded formatting issues, axis orientation, legend titles etc... The main thing i learned through the first half of the quarter is what resources can properly help me revise my thinking and what resources are simply stunting my growth in this class. I think my use of ChatGPT at the beginning of this class was an issue, but now after going through my previous work, and the lectures, i feel confident and able to progress in this class. Being able to do revisions and get feedback is something incredible in this class. Through revisions, i start by highlighting everything that i messed up or need growth on. I go back to my lecture notes and slides, as well as the readings and videos and the website to help me learn from the mistake and not just blindly correct it.

<!-- For the revisions included in your Portfolio, to help me understand the nature of your revisions, please denote somehow the feedback I provided you (e.g., boldface, italics, colored text) before your revisions. -->

## Extending My Thinking

<!-- How did you extended your thinking throughout the course? How did you extend your thinking on the code examples you have provided in your portfolio? -->

In the first half of this class I was able to extend my thinking by learning how to combine different R skills together in more practical ways, analyzing data and creating meaningful graphs with the data. At first, I focused on just getting code to run, as that was my main priority and i wouldn't really look at my code in detail to see where i could make it more efficient. Yet over these past few weeks I started to think more about how each step connects, for example, using pipes to link data wrangling and visualization, or creating new variables to make plots more meaningful. I also learned to look for more efficient and readable solutions, like using functions and joins instead of repeating code. Overall, I moved from following examples to actually understanding why I was using certain tools, which helped me apply them confidently in my own analyses. I understand that my portfolio is less complete then others for where we are in class but i am confident that I will have a full successful portfolio by the end of our class. Since I had my concussion and had one Lab dismissed I have less work to show. Nonetheless these are just a few bumps in the road and i am ready to continue to extend my thinking in this class and demonstrate my understanding of the work we have done in class.

## Peer Support & Collaboration

<!-- Include an image or a description of feedback you gave that you are proud of (either in a peer review or in Discord). -->

For my Lab 4 code review i was able to really understand the code i was looking at and what was expected of us. I think being able peer review other students work has been extremely helpful in developing my own skills in the class. During this peer review i submitted my review of my peer and made sure to include largely positives as a way to encourage my peer but also i appreciated the effort they put in there work. Something i else focused on doing was not saying that any of there work was wrong but more so something my peer can improve on, and i think tats key into making a supportive and but also constructive peer review that helps build growth in my peers but also my assessment of others work.

<!-- Include a description of how you grew as a collaborator through the weekly pair programming activities.   -->

I think I grew as a collaborator during our practice activities in many different ways. The way the practice activities are formatted are super helpful in terms of both participants being able to share there ideas. I sometimes struggle with sharing my ideas in group or partner settings in fear or saying the wrong the thing, but i feel the way our practice activities are formatted make me more comfortable sharing ideas even if there wrong and having my partner help me by asking leading questions to my thinking. The same works vice versa, i am also able to develop my thinking by problem solving and only asking aiding questions to my partner whilst they try and find a solution to the issue at hand. This helps me become a better and more equal peer collaborator, as we are able to have both of our ideas shared and communicated, whilst still feeling heard.
